From e6f982b439945a3862ea552951bc3e44c8de437b Mon Sep 17 00:00:00 2001
From: Chris Goffinet <cg@chrisgoffinet.com>
Date: Mon, 20 Jul 2009 17:48:15 -0700
Subject: [PATCH 3/4] Patches from Dhruba Borthakur

---
 src/c++/libhdfs/hdfs.c                             |  218 ++++++++++++++++++++
 src/c++/libhdfs/hdfs.h                             |   17 ++-
 src/c++/libhdfs/hdfs_test.c                        |    6 +-
 src/core/org/apache/hadoop/fs/FileSystem.java      |   66 ++++++-
 src/core/org/apache/hadoop/ipc/Client.java         |    4 +-
 src/hdfs/org/apache/hadoop/hdfs/DFSClient.java     |   12 +-
 .../hadoop/hdfs/server/namenode/FSNamesystem.java  |   19 ++
 7 files changed, 332 insertions(+), 10 deletions(-)

diff --git a/src/c++/libhdfs/hdfs.c b/src/c++/libhdfs/hdfs.c
index 59b2e80..20a5de4 100644
--- a/src/c++/libhdfs/hdfs.c
+++ b/src/c++/libhdfs/hdfs.c
@@ -343,7 +343,194 @@ hdfsFS hdfsConnectAsUser(const char* host, tPort port, const char *user , const
     return gFsRef;
 }
 
+/** Always return a new FileSystem handle */
+hdfsFS hdfsConnectNewInstance(const char* host, tPort port) {
+  // conect with NULL as user name/groups
+  return hdfsConnectAsUserNewInstance(host, port, NULL, NULL, 0);
+}
+
+
+/** Always return a new FileSystem handle */
+hdfsFS hdfsConnectAsUserNewInstance(const char* host, tPort port, const char *user , const char **groups, int groups_size )
+{
+    // JAVA EQUIVALENT:
+    //  FileSystem fs = FileSystem.get(new Configuration());
+    //  return fs;
+
+    JNIEnv *env = 0;
+    jobject jConfiguration = NULL;
+    jobject jFS = NULL;
+    jobject jURI = NULL;
+    jstring jURIString = NULL;
+    jvalue  jVal;
+    jthrowable jExc = NULL;
+    char    *cURI = 0;
+    jobject gFsRef = NULL;
+
+
+    //Get the JNIEnv* corresponding to current thread
+    env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return NULL;
+    }
+
+    //Create the org.apache.hadoop.conf.Configuration object
+    jConfiguration =
+        constructNewObjectOfClass(env, NULL, HADOOP_CONF, "()V");
+
+    if (jConfiguration == NULL) {
+        fprintf(stderr, "Can't construct instance of class "
+                "org.apache.hadoop.conf.Configuration\n");
+        errno = EINTERNAL;
+        return NULL;
+    }
+ 
+    if (user != NULL) {
+
+      if (groups == NULL || groups_size <= 0) {
+        fprintf(stderr, "ERROR: groups must not be empty/null\n");
+        errno = EINVAL;
+        return NULL;
+      }
+
+      jstring jUserString = (*env)->NewStringUTF(env, user);
+      jarray jGroups = constructNewArrayString(env, &jExc, groups, groups_size);
+      if (jGroups == NULL) {
+        errno = EINTERNAL;
+        fprintf(stderr, "ERROR: could not construct groups array\n");
+        return NULL;
+      }
+
+      jobject jUgi;
+      if ((jUgi = constructNewObjectOfClass(env, &jExc, HADOOP_UNIX_USER_GROUP_INFO, JMETHOD2(JPARAM(JAVA_STRING), JARRPARAM(JAVA_STRING), JAVA_VOID), jUserString, jGroups)) == NULL) {
+        fprintf(stderr,"failed to construct hadoop user unix group info object\n");
+        errno = errnoFromException(jExc, env, HADOOP_UNIX_USER_GROUP_INFO,
+                                   "init");
+        destroyLocalReference(env, jConfiguration);
+        destroyLocalReference(env, jUserString);
+        if (jGroups != NULL) {
+          destroyLocalReference(env, jGroups);
+        }          
+        return NULL;
+      }
+#define USE_UUGI
+#ifdef USE_UUGI
+
+      // UnixUserGroupInformation.UGI_PROPERTY_NAME
+      jstring jAttrString = (*env)->NewStringUTF(env,"hadoop.job.ugi");
+      
+      if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_UNIX_USER_GROUP_INFO, "saveToConf",
+                       JMETHOD3(JPARAM(HADOOP_CONF), JPARAM(JAVA_STRING), JPARAM(HADOOP_UNIX_USER_GROUP_INFO), JAVA_VOID),
+                       jConfiguration, jAttrString, jUgi) != 0) {
+        errno = errnoFromException(jExc, env, HADOOP_FSPERM,
+                                   "init");
+        destroyLocalReference(env, jConfiguration);
+        destroyLocalReference(env, jUserString);
+        if (jGroups != NULL) {
+          destroyLocalReference(env, jGroups);
+        }          
+        destroyLocalReference(env, jUgi);
+        return NULL;
+      }
+
+      destroyLocalReference(env, jUserString);
+      destroyLocalReference(env, jGroups);
+      destroyLocalReference(env, jUgi);
+    }
+#else
+    
+    // what does "current" mean in the context of libhdfs ? does it mean for the last hdfs connection we used?
+    // that's why this code cannot be activated. We know the above use of the conf object should work well with 
+    // multiple connections.
+      if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_USER_GROUP_INFO, "setCurrentUGI",
+                       JMETHOD1(JPARAM(HADOOP_USER_GROUP_INFO), JAVA_VOID),
+                       jUgi) != 0) {
+        errno = errnoFromException(jExc, env, HADOOP_USER_GROUP_INFO,
+                                   "setCurrentUGI");
+        destroyLocalReference(env, jConfiguration);
+        destroyLocalReference(env, jUserString);
+        if (jGroups != NULL) {
+          destroyLocalReference(env, jGroups);
+        }          
+        destroyLocalReference(env, jUgi);
+        return NULL;
+      }
+
+      destroyLocalReference(env, jUserString);
+      destroyLocalReference(env, jGroups);
+      destroyLocalReference(env, jUgi);
+    }
+#endif      
+    //Check what type of FileSystem the caller wants...
+    if (host == NULL) {
+        // fs = FileSytem::createLocalNewInstance(conf);
+        if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS, "createLocalNewInstance",
+                         JMETHOD1(JPARAM(HADOOP_CONF),
+                                  JPARAM(HADOOP_LOCALFS)),
+                         jConfiguration) != 0) {
+            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                       "FileSystem::createLocalNewInstance");
+            goto done;
+        }
+        jFS = jVal.l;
+    }
+    else if (!strcmp(host, "default") && port == 0) {
+        //fs = FileSystem::createNewInstance(conf); 
+        if (invokeMethod(env, &jVal, &jExc, STATIC, NULL,
+                         HADOOP_FS, "createNewInstance",
+                         JMETHOD1(JPARAM(HADOOP_CONF),
+                                  JPARAM(HADOOP_FS)),
+                         jConfiguration) != 0) {
+            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                       "FileSystem::createNewInstance");
+            goto done;
+        }
+        jFS = jVal.l;
+    }
+    else {
+        // fs = FileSystem::createNewInstance(URI, conf);
+        cURI = malloc(strlen(host)+16);
+        sprintf(cURI, "hdfs://%s:%d", host, (int)(port));
+
+        jURIString = (*env)->NewStringUTF(env, cURI);
+        if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, JAVA_NET_URI,
+                         "create", "(Ljava/lang/String;)Ljava/net/URI;",
+                         jURIString) != 0) {
+            errno = errnoFromException(jExc, env, "java.net.URI::create");
+            goto done;
+        }
+        jURI = jVal.l;
+
+        if (invokeMethod(env, &jVal, &jExc, STATIC, NULL, HADOOP_FS, "createNewInstance",
+                         JMETHOD2(JPARAM(JAVA_NET_URI),
+                                  JPARAM(HADOOP_CONF), JPARAM(HADOOP_FS)),
+                         jURI, jConfiguration) != 0) {
+            errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                       "Filesystem::createNewInstance(URI, Configuration)");
+            goto done;
+        }
 
+        jFS = jVal.l;
+    }
+
+  done:
+    
+    // Release unnecessary local references
+    destroyLocalReference(env, jConfiguration);
+    destroyLocalReference(env, jURIString);
+    destroyLocalReference(env, jURI);
+
+    if (cURI) free(cURI);
+
+    /* Create a global reference for this fs */
+    if (jFS) {
+        gFsRef = (*env)->NewGlobalRef(env, jFS);
+        destroyLocalReference(env, jFS);
+    }
+
+    return gFsRef;
+}
 
 int hdfsDisconnect(hdfsFS fs)
 {
@@ -919,7 +1106,38 @@ int hdfsFlush(hdfsFS fs, hdfsFile f)
     return 0;
 }
 
+int hdfsSync(hdfsFS fs, hdfsFile f) 
+{
+    // JAVA EQUIVALENT
+    //  FSDataOutputStream.sync
+
+    //Get the JNIEnv* corresponding to current thread
+    JNIEnv* env = getJNIEnv();
+    if (env == NULL) {
+      errno = EINTERNAL;
+      return -1;
+    }
+
+    //Parameters
+    jobject jOutputStream = (jobject)(f ? f->file : 0);
+
+    //Caught exception
+    jthrowable jExc = NULL;
 
+    //Sanity check
+    if (!f || f->type != OUTPUT) {
+        errno = EBADF;
+        return -1;
+    }
+
+    if (invokeMethod(env, NULL, &jExc, INSTANCE, jOutputStream, 
+                     HADOOP_OSTRM, "sync", "()V") != 0) {
+        errno = errnoFromException(jExc, env, "org.apache.hadoop.fs."
+                                   "FSDataOutputStream::sync");
+        return -1;
+    }
+    return 0;
+}
 
 int hdfsAvailable(hdfsFS fs, hdfsFile f)
 {
diff --git a/src/c++/libhdfs/hdfs.h b/src/c++/libhdfs/hdfs.h
index b57b42e..4bc3300 100644
--- a/src/c++/libhdfs/hdfs.h
+++ b/src/c++/libhdfs/hdfs.h
@@ -90,7 +90,6 @@ extern  "C" {
     };
     typedef struct hdfsFile_internal* hdfsFile;
       
-
     /** 
      * hdfsConnectAsUser - Connect to a hdfs file system as a specific user
      * Connect to the hdfs.
@@ -121,6 +120,14 @@ extern  "C" {
      hdfsFS hdfsConnect(const char* host, tPort port);
 
 
+    /**
+     * This are the same as hdfsConnectAsUser except that every invocation returns a new FileSystem handle.
+     * Applications should call a hdfsDisconnect for every call to hdfsConnectAsUserNewInstance.
+     */
+     hdfsFS hdfsConnectAsUserNewInstance(const char* host, tPort port, const char *user , const char *groups[], int groups_size );
+     hdfsFS hdfsConnectNewInstance(const char* host, tPort port);
+     hdfsFS hdfsConnectPath(const char* uri);
+
     /** 
      * hdfsDisconnect - Disconnect from the hdfs file system.
      * Disconnect from hdfs.
@@ -232,6 +239,14 @@ extern  "C" {
      */
     int hdfsFlush(hdfsFS fs, hdfsFile file);
 
+    /** 
+     * hdfsSync - Sync the data to persistent store.
+     * @param fs The configured filesystem handle.
+     * @param file The file handle.
+     * @return Returns 0 on success, -1 on error. 
+     */
+    int hdfsSync(hdfsFS fs, hdfsFile file);
+
 
     /**
      * hdfsAvailable - Number of bytes that can be read from this
diff --git a/src/c++/libhdfs/hdfs_test.c b/src/c++/libhdfs/hdfs_test.c
index a26cc82..1d2bd51 100644
--- a/src/c++/libhdfs/hdfs_test.c
+++ b/src/c++/libhdfs/hdfs_test.c
@@ -52,13 +52,13 @@ void permission_disp(short permissions, char *rtr) {
 
 int main(int argc, char **argv) {
 
-    hdfsFS fs = hdfsConnect("default", 0);
+    hdfsFS fs = hdfsConnectNewInstance("default", 0);
     if(!fs) {
         fprintf(stderr, "Oops! Failed to connect to hdfs!\n");
         exit(-1);
     } 
  
-    hdfsFS lfs = hdfsConnect(NULL, 0);
+    hdfsFS lfs = hdfsConnectNewInstance(NULL, 0);
     if(!lfs) {
         fprintf(stderr, "Oops! Failed to connect to 'local' hdfs!\n");
         exit(-1);
@@ -401,7 +401,7 @@ int main(int argc, char **argv) {
       groups[0] = "users";
       groups[1] = "nobody";
 
-      fs = hdfsConnectAsUser("default", 0, tuser, groups, 2);
+      fs = hdfsConnectAsUserNewInstance("default", 0, tuser, groups, 2);
       if(!fs) {
         fprintf(stderr, "Oops! Failed to connect to hdfs as user %s!\n",tuser);
         exit(-1);
diff --git a/src/core/org/apache/hadoop/fs/FileSystem.java b/src/core/org/apache/hadoop/fs/FileSystem.java
index 4d07500..aa37308 100644
--- a/src/core/org/apache/hadoop/fs/FileSystem.java
+++ b/src/core/org/apache/hadoop/fs/FileSystem.java
@@ -33,6 +33,7 @@ import java.util.Set;
 import java.util.TreeSet;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.regex.Pattern;
+import java.util.concurrent.atomic.AtomicLong;
 
 import javax.security.auth.login.LoginException;
 
@@ -191,6 +192,47 @@ public abstract class FileSystem extends Configured implements Closeable {
     return CACHE.get(uri, conf);
   }
 
+  /** Returns the FileSystem for this URI's scheme and authority.  The scheme
+   * of the URI determines a configuration property name,
+   * <tt>fs.<i>scheme</i>.class</tt> whose value names the FileSystem class.
+   * The entire URI is passed to the FileSystem instance's initialize method.
+   * This always returns a new FileSystem object.
+   */
+  public static FileSystem createNewInstance(URI uri, Configuration conf) throws IOException {
+    String scheme = uri.getScheme();
+    String authority = uri.getAuthority();
+
+    if (scheme == null) {                       // no scheme: use default FS
+      return createNewInstance(conf);
+    }
+
+    if (authority == null) {                       // no authority
+      URI defaultUri = getDefaultUri(conf);
+      if (scheme.equals(defaultUri.getScheme())    // if scheme matches default
+          && defaultUri.getAuthority() != null) {  // & default has authority
+        return createNewInstance(defaultUri, conf);              // return default
+      }
+    }
+    return CACHE.getUnique(uri, conf);
+  }
+
+  /** Returns a unique configured filesystem implementation.
+   * This always returns a new FileSystem object. */
+  public static FileSystem createNewInstance(Configuration conf) throws IOException {
+    return createNewInstance(getDefaultUri(conf), conf);
+  }
+
+  /**
+   * Get a unique local file system object
+   * @param conf the configuration to configure the file system with
+   * @return a LocalFileSystem
+   * This always returns a new FileSystem object.
+   */
+  public static LocalFileSystem createLocalNewInstance(Configuration conf)
+    throws IOException {
+    return (LocalFileSystem)createNewInstance(LocalFileSystem.NAME, conf);
+  }
+
   private static class ClientFinalizer extends Thread {
     public synchronized void run() {
       try {
@@ -1378,8 +1420,21 @@ public abstract class FileSystem extends Configured implements Closeable {
   static class Cache {
     private final Map<Key, FileSystem> map = new HashMap<Key, FileSystem>();
 
+    /** A variable that makes all objects in the cache unique */
+    private static AtomicLong unique = new AtomicLong(1);
+
     synchronized FileSystem get(URI uri, Configuration conf) throws IOException{
       Key key = new Key(uri, conf);
+      return getInternal(uri, conf, key);
+    }
+
+    /** The objects inserted into the cache using this method are all unique */
+    synchronized FileSystem getUnique(URI uri, Configuration conf) throws IOException{
+      Key key = new Key(uri, conf, unique.getAndIncrement());
+      return getInternal(uri, conf, key);
+    }
+
+    private FileSystem getInternal(URI uri, Configuration conf, Key key) throws IOException{
       FileSystem fs = map.get(key);
       if (fs == null) {
         fs = createFileSystem(uri, conf);
@@ -1434,10 +1489,16 @@ public abstract class FileSystem extends Configured implements Closeable {
       final String scheme;
       final String authority;
       final String username;
+      final long unique;   // an artificial way to make a key unique
 
       Key(URI uri, Configuration conf) throws IOException {
+        this(uri, conf, 0);
+      }
+
+      Key(URI uri, Configuration conf, long unique) throws IOException {
         scheme = uri.getScheme()==null?"":uri.getScheme().toLowerCase();
         authority = uri.getAuthority()==null?"":uri.getAuthority().toLowerCase();
+        this.unique = unique;
         UserGroupInformation ugi = UserGroupInformation.readFrom(conf);
         if (ugi == null) {
           try {
@@ -1451,7 +1512,7 @@ public abstract class FileSystem extends Configured implements Closeable {
 
       /** {@inheritDoc} */
       public int hashCode() {
-        return (scheme + authority + username).hashCode();
+        return (scheme + authority + username).hashCode() + (int)unique;
       }
 
       static boolean isEqual(Object a, Object b) {
@@ -1467,7 +1528,8 @@ public abstract class FileSystem extends Configured implements Closeable {
           Key that = (Key)obj;
           return isEqual(this.scheme, that.scheme)
                  && isEqual(this.authority, that.authority)
-                 && isEqual(this.username, that.username);
+                 && isEqual(this.username, that.username)
+                 && (this.unique == that.unique);
         }
         return false;        
       }
diff --git a/src/core/org/apache/hadoop/ipc/Client.java b/src/core/org/apache/hadoop/ipc/Client.java
index 914c7e4..f9a753e 100644
--- a/src/core/org/apache/hadoop/ipc/Client.java
+++ b/src/core/org/apache/hadoop/ipc/Client.java
@@ -73,6 +73,7 @@ public class Client {
   final private int maxRetries; //the max. no. of retries for socket connections
   private boolean tcpNoDelay; // if T then disable Nagle's Algorithm
   private int pingInterval; // how often sends ping to the server in msecs
+  private int socketConnectWaitTime; // how long to block for a connection request to server
   final private boolean doPing; //do we need to send ping message
 
   private SocketFactory socketFactory;           // how to create sockets
@@ -318,7 +319,7 @@ public class Client {
             this.socket = socketFactory.createSocket();
             this.socket.setTcpNoDelay(tcpNoDelay);
             // connection time out is 20s
-            NetUtils.connect(this.socket, remoteId.getAddress(), 20000);
+            NetUtils.connect(this.socket, remoteId.getAddress(), socketConnectWaitTime);
             this.socket.setSoTimeout(pingInterval);
             break;
           } catch (SocketTimeoutException toe) {
@@ -654,6 +655,7 @@ public class Client {
     this.tcpNoDelay = conf.getBoolean("ipc.client.tcpnodelay", false);
     this.doPing = conf.getBoolean("ipc.client.ping", true);
     this.pingInterval = getPingInterval(conf);
+    this.socketConnectWaitTime = conf.getInt("ipc.client.connect.maxwaittime", 20000);
     if (LOG.isDebugEnabled()) {
       LOG.debug("The ping interval is" + this.pingInterval + "ms.");
     }
diff --git a/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java b/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
index 8e995d4..2adb3cf 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
@@ -1047,6 +1047,7 @@ public class DFSClient implements FSConstants, java.io.Closeable {
      */
     public void run() {
       long lastRenewed = 0;
+      int leaseFailureMax = conf.getInt("dfs.leaserenewal.timeout", 0);
       int renewal = (int)(LEASE_SOFTLIMIT_PERIOD / 2);
       if (hdfsTimeout > 0) {
         renewal = Math.min(renewal, hdfsTimeout/2);
@@ -1063,9 +1064,14 @@ public class DFSClient implements FSConstants, java.io.Closeable {
             abort();
             break;
           } catch (IOException ie) {
-            LOG.warn("Problem renewing lease for " + clientName +
-                     " for a period of " + (hdfsTimeout/1000) +
-                     " seconds. Will retry shortly...", ie);
+            if (leaseFailureMax != 0 &&
+                System.currentTimeMillis()  - lastRenewed > leaseFailureMax) {
+                LOG.warn("Problem renewing lease for " + clientName + " for time " +
+                       leaseFailureMax + "ms. Aborting...", ie);
+              clientRunning = false;
+            } else {
+              LOG.warn("Problem renewing lease for " + clientName, ie);            
+            }
           }
         }
 
diff --git a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
index 7add7ea..e5011ee 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java
@@ -824,6 +824,25 @@ public class FSNamesystem implements FSConstants, FSNamesystemMBean {
             machineSet[numNodes++] = dn;
         }
       }
+      // If the inode is under construction and we are processing the last
+      // block, then retrieve its locations in a special way. This block
+      // might not yet be in the BlocksMap because the size of the last block is known
+      // only to the writer. Return only the first location so that the client
+      // does not see different file size while accessing multiple replicas.
+      if (inode.isUnderConstruction()) {
+        LOG.warn("XXX curblk " + curBlk +
+                 " blocks.length " + blocks.length +
+                 " numMachineSet " + numMachineSet);
+      }
+      if (inode.isUnderConstruction() && curBlk == blocks.length - 1 &&
+          numMachineSet == 0) {
+        DatanodeDescriptor[] targets = ((INodeFileUnderConstruction)inode).getTargets();
+        if (targets != null && targets.length >= 1) {
+          machineSet = new DatanodeDescriptor[1];
+          machineSet[0] = targets[0];
+          LOG.warn("XXX targets[0] = " + machineSet[0]);
+        }
+      }
       results.add(new LocatedBlock(blocks[curBlk], machineSet, curPos,
                   blockCorrupt));
       curPos += blocks[curBlk].getNumBytes();
-- 
1.6.3.1

